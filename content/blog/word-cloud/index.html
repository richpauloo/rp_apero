---
title: Using tidytext to make word clouds
author: Rich
summary: How to make a word cloud in R. In this example, I use the research prospectus I just submitted in preparation for my qualifying examination.
date: '2017-12-29 02:57:45+00:00'
slug: word-cloud
header:
  caption: ''
  image: ''
image: 
  caption: ""
projects: ""
---



<blockquote>
<p>I’ve since turned this code into a simple <a href="https://richpauloo.shinyapps.io/word_cloud_app">Shiny app that makes word clouds out of .txt files</a>.</p>
</blockquote>
<p>I recently came across the <code>tidytext</code> R package, and the accompanying book, <a href="https://www.tidytextmining.com/">Text Mining in R</a> by David Robinson and Julia Silge. I found it very practical for basic text mining and NLP problems spanning tf, idf, tf-idf, word vectorization, cosine similarity, sentiment analysis, and topic modeling.</p>
<p>There are a pleathora of out-of-the-box functions and data that help with common natural language processing (NLP) tasks, like:</p>
<ul>
<li>tokenizing documents</li>
<li>removing stop-words (e.g. - a, an, and, the, but)</li>
<li>calculating tf, idf, and tf-idf</li>
</ul>
<p>After playing around a bit with examples, I thought it would be interesting to see what the 38 page research prospectus I spent months slaving over boiled down to in terms of term frequency.</p>
<hr />
<div id="bring-in-data" class="section level1">
<h1>Bring in Data</h1>
<p>I first saved my <em>.docx</em> file as a <em>.txt</em> in <em>UTF-8</em> encoding because it’s easier for R to read. The result is a very messy table, which I won’t print here.</p>
</div>
<div id="load-libraries-and-read-data" class="section level1">
<h1>Load libraries and read data</h1>
<pre class="r"><code>library(dplyr) # for data wrangling
library(tidytext) # for NLP
library(stringr) # to deal with strings
library(wordcloud) # to render wordclouds
library(knitr) # for tables
library(tidyr)

# the local file path to my research prospectus
path &lt;- &#39;~/Documents/Github/rp/static/data/rp.txt&#39; 

# fill = TRUE b/c rows are of unequal length
dat &lt;- read.table(path, header = FALSE, fill = TRUE) </code></pre>
</div>
<div id="tidy" class="section level1">
<h1>1. Tidy</h1>
<p>Since the package we’re using adheres to tidy data principles, step 1 is to get this messy table into a one column data frame, with one word in each row.</p>
<pre class="r"><code># reshape the .txt data frame into one column
tidy_dat &lt;- tidyr::gather(dat, key, word) %&gt;% select(word)

tidy_dat$word %&gt;% length() # there are 10,480 tokens in my document</code></pre>
<pre><code>## [1] 10480</code></pre>
<pre class="r"><code>unique(tidy_dat$word) %&gt;% length() # and of these, 2,866 are unique </code></pre>
<pre><code>## [1] 2866</code></pre>
</div>
<div id="tokenize" class="section level1">
<h1>2. Tokenize</h1>
<p>The next step is to <em>tokenize</em>, or boil the dataframe down down to only unique observations, and count the number of each observation. To perform this, we use <code>unnest_tokens()</code>, which takes 3 arguments:</p>
<ul>
<li>a tidy data frame</li>
<li>name of the output column to be created</li>
<li>name of the input column to be split into tokens</li>
</ul>
<p>Then we use the <code>count()</code> function from <code>dplyr</code> to group by words and tally observations. Becauase <code>count()</code> performs a <code>group_by()</code> on the <em>word</em> column, we <code>ungroup()</code>.</p>
<pre class="r"><code># tokenize
tokens &lt;- tidy_dat %&gt;% 
  unnest_tokens(word, word) %&gt;% 
  count(word, sort = TRUE) %&gt;% 
  ungroup()</code></pre>
<p>Just because a token is common doesn’t mean it’s important. For instance, take a look at the most 10 common tokens in my research prospectus.</p>
<pre class="r"><code>tokens %&gt;% head(10)</code></pre>
<pre><code>##           word   n
## 1          the 487
## 2          and 368
## 3           of 344
## 4           in 235
## 5           to 235
## 6  groundwater 193
## 7        water 138
## 8            a 137
## 9           is 112
## 10         for  92</code></pre>
<p>Of the 10, only 2 actually tell us something about what’s written about: <em>groundwater</em>, and <em>water</em>. Cleaning natural language is like panning for gold: most of language is useless, but every once in a while we find a gold nugget. We want to get only the nuggets.</p>
{{% figure src="/img/gold.jpg" title="This is you tokenizing." %}}
</div>
<div id="remove-stop-words-numbers-etc." class="section level1">
<h1>3. Remove Stop Words, Numbers, Etc.</h1>
<p><code>tidytext</code> has some built-in libraries of stop words. We’ll use an <code>anti_join()</code> to get rid of stop words anc clean our tokens.</p>
<pre class="r"><code># remove stop words
data(&quot;stop_words&quot;)
tokens_clean &lt;- tokens %&gt;%
  anti_join(stop_words)</code></pre>
<pre><code>## Joining, by = &quot;word&quot;</code></pre>
<p>While we’re at it, we’ll use a regex to clean all numbers.</p>
<pre class="r"><code># remove numbers
nums &lt;- tokens_clean %&gt;% filter(str_detect(word, &quot;^[0-9]&quot;)) %&gt;% select(word) %&gt;% unique()

tokens_clean &lt;- tokens_clean %&gt;% 
  anti_join(nums, by = &quot;word&quot;)</code></pre>
<p>I also did a quick pass over <code>tokens_clean</code> to look for other meaningless tokens that escaped the stop-word dictionary and the numbers. It’s not surprising that the tokens that made it by were:</p>
<ul>
<li>al - from citations (e.g. - et. al)</li>
<li>figure - figure captions and references in my prospectus</li>
<li>i.e. - gotta love those parentheticals</li>
<li>etc…</li>
</ul>
<p>I’ll store these unique stop words in a vector and perform another <code>anti_join</code>, et voila. A tidy, clean list of tokens and counts.</p>
<pre class="r"><code># remove unique stop words that snuck in there
uni_sw &lt;- data.frame(word = c(&quot;al&quot;,&quot;figure&quot;,&quot;i.e&quot;, &quot;l3&quot;))

tokens_clean &lt;- tokens_clean %&gt;% 
  anti_join(uni_sw, by = &quot;word&quot;)</code></pre>
</div>
<div id="make-a-word-cloud-of-the-top-50-words" class="section level1">
<h1>4. Make a Word Cloud of the top 50 words</h1>
<p>And just like that, an easy word cloud. In fact, this code was so simple and fun that I wrapped it into a <a href="https://richpauloo.shinyapps.io/word_cloud_app">Shiny App</a>.</p>
<pre class="r"><code># define a nice color palette
pal &lt;- brewer.pal(8,&quot;Dark2&quot;)

# plot the 50 most common words
tokens_clean %&gt;% 
  with(wordcloud(word, n, random.order = FALSE, max.words = 50, colors=pal))</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
{{% figure src="/img/YES.gif" %}}
</div>
